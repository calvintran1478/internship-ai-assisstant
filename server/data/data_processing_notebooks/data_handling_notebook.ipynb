{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b691db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\sande\\OneDrive\\Bureau\\UofT\\CSC2701_Communication4CS\\internship-ai-assisstant\\server\\data\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import os.path as op\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "RAW_DATA_DIR = \"raw_data\"\n",
    "OUTPUT_DATA_DIR = \"processed_data\"\n",
    "\n",
    "# If the last part of the path of the current working directory is \"data_process_notebooks\", change to the parent directory (the main one for all data-related)\n",
    "current_dir = os.getcwd()\n",
    "if op.basename(current_dir) == \"data_processing_notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc25afd",
   "metadata": {},
   "source": [
    "## Part 1 : PDFs\n",
    "\n",
    "Steps:\n",
    "- Extract all info from the PDFs\n",
    "- Clean the text\n",
    "- Merging individual pages and chunk intelligently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e493b36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sande\\anaconda3\\envs\\CSC2701Env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "c:\\Users\\sande\\anaconda3\\envs\\CSC2701Env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports for this part\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e33255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 181 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 186 0 (offset 0)\n",
      "Ignoring wrong pointing object 188 0 (offset 0)\n",
      "Ignoring wrong pointing object 190 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 197 0 (offset 0)\n",
      "Ignoring wrong pointing object 478 0 (offset 0)\n",
      "Ignoring wrong pointing object 479 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all pdf document names in the ProjData folder\n",
    "pdf_filenames = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith(\".pdf\")]\n",
    "# For storing loaded documents\n",
    "docs = []\n",
    "# Extraction\n",
    "for pdf_filename in pdf_filenames:\n",
    "    pdf_path = op.join(\"raw_data\", pdf_filename)\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    docs.extend(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6019696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for text cleanup\n",
    "def clean_text(text):\n",
    "    # Remove multiple newlines\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove hyphenation at line breaks (e.g. \"inter-\\nview\" → \"interview\")\n",
    "    text = re.sub(r'(\\w)-\\s+(\\w)', r'\\1\\2', text)\n",
    "    # Strip weird bullet points or artifacts\n",
    "    text = re.sub(r'•|▪|●|–|­', '', text)\n",
    "    # Normalize whitespace and punctuation even more\n",
    "    text = re.sub(r'\\s([?.!,:;])', r'\\1', text)\n",
    "    # Trim spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Clean up the text in each document\n",
    "for doc in docs:\n",
    "    doc.page_content = clean_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5315a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all text per PDF\n",
    "pdf_texts = defaultdict(str)\n",
    "for doc in docs:\n",
    "    pdf_texts[doc.metadata[\"source\"]] += \" \" + doc.page_content\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "clean_pdf_docs = []\n",
    "for pdf_name, text in pdf_texts.items():\n",
    "    for chunk in splitter.create_documents([text]):\n",
    "        chunk.metadata[\"source\"] = pdf_name\n",
    "        clean_pdf_docs.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c377c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'raw_data\\\\Luki_Lec1.pdf'}\n",
      "2701: Section 5301 Elevate your Interviewing and networking Benchmark and Level set 1 Benchmark and Level set 2 6:00 pm Introductions, Foundations for interview and networking + Benchmark 7:00 pm BREAK 7:15 pm LinkedIn +Networking 8:00 pm BREAK 8:15 pm Interview preparation 9:00 pm Thank you! Objectives: After today’s session you should  Understand how to land an internship  Have strong foundations for interviews and networking  Have a better understanding of your interview and networking\n",
      "——————————————————————————————————————————————————\n",
      "{'source': 'raw_data\\\\Luki_Lec1.pdf'}\n",
      "for interviews and networking  Have a better understanding of your interview and networking levels and areas to improve Luki Danukarjanto 3  Career Coach and Educator  Former Computer Scientist turned Management Consultant (+ campus recruiting lead) turned Educator/Entrepreneur  Author and Podcast/YouTube host “SIWIKE: Stuff I Wish I Knew Earlier” This is a safe space! Plus a BRAVE space! 4 We’re here to learn and grow: Progress vs Perfection! Get out of your comfort zone! 5 ANXIETY ZONE\n",
      "——————————————————————————————————————————————————\n",
      "{'source': 'raw_data\\\\Luki_Lec1.pdf'}\n",
      "We’re here to learn and grow: Progress vs Perfection! Get out of your comfort zone! 5 ANXIETY ZONE COMFORT ZONE LEARNING ZONE  If you’re feeling uncomfortable, you’re learning!  Let us know if you have gone into the “anxiety zone”!  Mistakes are not just expected, mistakes are encouraged! Tell me about yourself (in 30 seconds or less) 6  Name  Fun fact ○ Something most people don’t know about you ○ Favourite food, place, movie, music, game, etc.  What you’d like to get from these interview and\n",
      "——————————————————————————————————————————————————\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "for d in clean_pdf_docs[:3]:\n",
    "    print(d.metadata)\n",
    "    print(d.page_content)\n",
    "    print(\"—\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c033408",
   "metadata": {},
   "source": [
    "## Part 2 : Powerpoint presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e602ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4a0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all ppt document names in the ProjData folder\n",
    "ppt_filenames = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith((\".ppt\", \".pptx\"))]\n",
    "# Helper for extraction\n",
    "def extract_ppt_text(path):\n",
    "    prs = Presentation(path)\n",
    "    slides_text = []\n",
    "    \n",
    "    for i, slide in enumerate(prs.slides, start=1):\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                slide_text.append(shape.text.strip())\n",
    "        text = \" \".join(slide_text)\n",
    "        slides_text.append({\n",
    "            \"id\": f\"{os.path.basename(path)}_slide_{i}\",\n",
    "            \"text\": text,\n",
    "            \"metadata\": {\n",
    "                \"source\": os.path.basename(path),\n",
    "                \"slide_number\": i,\n",
    "                \"type\": \"ppt\"\n",
    "            }\n",
    "        })\n",
    "    return slides_text\n",
    "# Extraction\n",
    "ppt_docs = []\n",
    "for ppt_filename in ppt_filenames:\n",
    "    ppt_path = op.join(RAW_DATA_DIR, ppt_filename)\n",
    "    slides = extract_ppt_text(ppt_path)\n",
    "    for slide in slides:\n",
    "        ppt_docs.append(slide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6bd8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text in each ppt document\n",
    "for doc in ppt_docs:\n",
    "    doc[\"text\"] = clean_text(doc[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40317f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechunking\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "clean_ppt_docs = []\n",
    "for d in ppt_docs:\n",
    "    chunks = splitter.split_text(d[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        clean_ppt_docs.append({\n",
    "            \"id\": f'{d[\"id\"]}_chunk_{i}',\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": d[\"metadata\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51fe110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'MScAC_JobFindingStrategies.pptx', 'slide_number': 1, 'type': 'ppt'}\n",
      "Job Finding Strategies Presented by: Murtuza Rajkotwala Research and Business Development Officer September 25th, 2025 1\n",
      "--------------------------------------------------\n",
      "{'source': 'MScAC_JobFindingStrategies.pptx', 'slide_number': 2, 'type': 'ppt'}\n",
      "Introduction Welcome to MScAC! 2\n",
      "--------------------------------------------------\n",
      "{'source': 'MScAC_JobFindingStrategies.pptx', 'slide_number': 3, 'type': 'ppt'}\n",
      "Agenda Reminder Requirements for Internships Finding jobs through LinkedIn External Job Boards ATS and External Job Boards Web Scraping Project Tips and Tricks Workshop\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "for doc in clean_ppt_docs[:3]:\n",
    "    print(doc[\"metadata\"])\n",
    "    print(doc[\"text\"][:200])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a828bf",
   "metadata": {},
   "source": [
    "## Part 3 : Word documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e94440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b3f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all word document names in the ProjData folder\n",
    "docx_filenames = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith((\".docx\"))]\n",
    "# Extraction helper\n",
    "def extract_docx_text(path):\n",
    "    doc = Document(path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():\n",
    "            full_text.append(para.text.strip())\n",
    "    return \" \".join(full_text)\n",
    "# Extract\n",
    "docx_docs = []\n",
    "for docx_filename in docx_filenames:\n",
    "    text = extract_docx_text(op.join(RAW_DATA_DIR, docx_filename))\n",
    "    docx_docs.append({\n",
    "        \"id\": docx_filename,\n",
    "        \"text\": text,\n",
    "        \"metadata\": {\n",
    "            \"source\": docx_filename,\n",
    "            \"type\": \"word\"\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d5c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the text in each document\n",
    "for doc in docx_docs:\n",
    "    doc[\"text\"] = clean_text(doc[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "285f2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechunk\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "clean_docx_docs = []\n",
    "for doc in docx_docs:\n",
    "    chunks = splitter.split_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        clean_docx_docs.append({\n",
    "            \"id\": f\"{doc['id']}_chunk_{i}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d09df68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'MScAC_InternshipProposalForm.docx', 'type': 'word'}\n",
      "Master of Science in Applied Computing (MScAC) Program: MScAC Internship Project Proposal Form 1. Proposing Organization Include the name of the organization, size of the organization, location of the\n",
      "--------------------------------------------------\n",
      "{'source': 'MScAC_InternshipProposalForm.docx', 'type': 'word'}\n",
      "of the main business of the organization. Enter your text here. 4. Problem Statement (about 100 words) State the main challenge the internship is designed to address. Explain why this problem is of re\n",
      "--------------------------------------------------\n",
      "{'source': 'MScAC_InternshipProposalForm.docx', 'type': 'word'}\n",
      "hoping for during the internship. What does success look like? If appropriate highlight how this success will impact your organization. Enter your text here. 6. Methodology (about 200 words) Explain t\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "for d in clean_docx_docs[:3]:\n",
    "    print(d[\"metadata\"])\n",
    "    print(d[\"text\"][:200])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb10fad",
   "metadata": {},
   "source": [
    "## Part 4 : MScAC calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e4cfbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ics import Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98e88850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all ICS filenames\n",
    "ics_filenames = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith((\".ics\"))]\n",
    "# Little check to make sure there is only one calendar, if not warn user there must only be one\n",
    "if len(ics_filenames) > 1:\n",
    "    print(\"Warning: More than one .ics file found. Please ensure there is only one calendar file. Here we only consider the 1st one.\")\n",
    "# Load the calendar\n",
    "with open(op.join(RAW_DATA_DIR, ics_filenames[0]), \"r\") as f:\n",
    "    calendar = Calendar(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb24958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get events\n",
    "events = []\n",
    "for i, event in enumerate(calendar.events):\n",
    "    events.append({\n",
    "        \"id\": f\"event_{i}\",\n",
    "        \"title\": event.name,\n",
    "        \"start\": event.begin.format('YYYY-MM-DD HH:mm'),\n",
    "        \"end\": event.end.format('YYYY-MM-DD HH:mm'),\n",
    "        \"location\": event.location if event.location else \"\",\n",
    "        \"description\": event.description if event.description else \"\"\n",
    "    })\n",
    "# Convert to text chunks for RAG\n",
    "for event in events:\n",
    "    text = f\"Event: {event['title']}\\n\"\n",
    "    text += f\"Date & Time: {event['start']} to {event['end']}\\n\"\n",
    "    if event['location']:\n",
    "        text += f\"Location: {event['location']}\\n\"\n",
    "    if event['description']:\n",
    "        text += f\"Description: {event['description']}\\n\"\n",
    "    event[\"text\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a2aba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group events per week to create chunks\n",
    "weekly_chunks = defaultdict(list)\n",
    "\n",
    "for event in events:\n",
    "    week = pd.to_datetime(event['start']).strftime(\"%Y-W%U\")\n",
    "    weekly_chunks[week].append(event[\"text\"])\n",
    "\n",
    "calendar_info = []\n",
    "for week, texts in weekly_chunks.items():\n",
    "    calendar_info.append({\n",
    "        \"id\": f\"events_{week}\",\n",
    "        \"text\": \"\\n---\\n\".join(texts),\n",
    "        \"metadata\": {\"week\": week, \"type\": \"calendar\"}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63edddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'week': '2025-W41', 'type': 'calendar'}\n",
      "Event: Movie Night Fridays [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-10-17 21:00 to 2025-10-18 00:00\n",
      "\n",
      "---\n",
      "Event:  Eva and Allen Lau Commercialization Catalyst Prize for Computing & Engineering Innovation [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-10-15 22:00 to 2025-10-16 00:00\n",
      "Location: TBC\n",
      "Description: Registration link: https://my.alumni.utoronto.ca/s/731/form-blank/index.aspx?sid=731&gid=1&pgid=25838&content_id=25265\n",
      "\n",
      "---\n",
      "Event: Industry Partner Afternoons: AMD [CSC2703H Y LEC5101]\n",
      "Date & Time: 202\n",
      "--------------------------------------------------\n",
      "{'week': '2025-W45', 'type': 'calendar'}\n",
      "Event: Movie Night Fridays [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-11-14 22:00 to 2025-11-15 01:00\n",
      "\n",
      "---\n",
      "Event: Industry Partner Afternoon: ModiFace [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-11-14 19:30 to 2025-11-14 22:00\n",
      "Location: 9014/9016\n",
      "\n",
      "---\n",
      "Event: MScACtalks: Yonatahn Kahn [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-11-11 16:00 to 2025-11-11 17:00\n",
      "\n",
      "---\n",
      "Event: ARIA [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-11-13 20:00 to 2025-11-14 00:00\n",
      "Location: Metro Toronto Convention Centre\n",
      "\n",
      "--------------------------------------------------\n",
      "{'week': '2025-W49', 'type': 'calendar'}\n",
      "Event: Movie Night Fridays [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-12-12 22:00 to 2025-12-13 01:00\n",
      "\n",
      "---\n",
      "Event: C.C. “Kelly” Gotlieb Distinguished Lecture Series 2025–2026: Anind Dey – HCI, ML & Mobile Computing [CSC2703H Y LEC5101]\n",
      "Date & Time: 2025-12-09 16:00 to 2025-12-09 17:30\n",
      "Location: BA3200, Bahen Centre for Information Technology\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "for d in calendar_info[:3]:\n",
    "    print(d[\"metadata\"])\n",
    "    print(d[\"text\"][:500])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed3e10b",
   "metadata": {},
   "source": [
    "## Part 5 : Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b00b71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b770c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from URLs\n",
    "URLs = [\n",
    "    \"https://www.cs.toronto.edu/dcs/documents/mscac/partner-guidelines/\"\n",
    "]\n",
    "webpage_info = []\n",
    "for url in URLs:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Remove scripts and styles\n",
    "    for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Get the main text\n",
    "    text = \" \".join(soup.stripped_strings)\n",
    "    # Clean the text\n",
    "    text = clean_text(text)\n",
    "    # Split into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        webpage_info.append({\n",
    "            \"id\": f\"{re.sub(r'\\\\W+', '_', url)}_chunk_{i}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source\": url,\n",
    "                \"type\": \"webpage\"\n",
    "            }\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c93638cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.cs.toronto.edu/dcs/documents/mscac/partner-guidelines/', 'type': 'webpage'}\n",
      "Recruiting Guidelines and Information for Partners | MScAC | University of Toronto Recruiting Guidelines and Information for Partners Welcome to MScAC Each graduate student in the Master of Science in Applied Computing program (MScAC) takes part in an 8-month internship (May to December) that must include a substantial applied research component. The MScAC graduate program imposes a higher standard of creative or intellectual exploration than would normally be encountered in a co-op study\n",
      "--------------------------------------------------\n",
      "{'source': 'https://www.cs.toronto.edu/dcs/documents/mscac/partner-guidelines/', 'type': 'webpage'}\n",
      "of creative or intellectual exploration than would normally be encountered in a co-op study program or a paid work experience program. Internship opportunities are usually paid and must be formally vetted by the MScAC program prior to being promoted to MScAC students. While MScAC does not have publication requirements, there are reporting requirements that should be reviewed by each prospective partner. Table of Contents Introduction to the Master of Science in Applied Computing (MScAC)\n",
      "--------------------------------------------------\n",
      "{'source': 'https://www.cs.toronto.edu/dcs/documents/mscac/partner-guidelines/', 'type': 'webpage'}\n",
      "partner. Table of Contents Introduction to the Master of Science in Applied Computing (MScAC) Overview of the MScAC Program Students Applied Research Internship Projects What Is an Applied Research Internship? Student Supervision Student Eligibility Intellectual Property Reporting Confidentiality Requirements ARIA Showcase and Final Report Computing Resources Program Timeline Onboarding of Prospective Partners and Project Submission Student Recruitment Internships Teaching Assistantships\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspection\n",
    "for d in webpage_info[:3]:\n",
    "    print(d[\"metadata\"])\n",
    "    print(d[\"text\"][:500])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7818947",
   "metadata": {},
   "source": [
    "## Part 6 : Additional MScAC-recommended web pages and resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "212463b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the metadata dictionary ourselves\n",
    "external_resources = [\n",
    "    {\n",
    "        \"id\": \"mscac_job_boards\",\n",
    "        \"type\": \"reference\",\n",
    "        \"text\": (\n",
    "            \"The MScAC team recommends checking out the following external job posting websites (c.f. the \\\"Internship and Academic Supervisor Information\\\" page of the CSC2703 page on Quercus for more details): \"\n",
    "            \"U of T Entrepreneurship job board, Vector Digital Talent Hub, Machine Learning Techniques Jobs, \"\n",
    "            \"MaRS Discovery District Job Board, Communitech Work In Tech, Toronto AI Job Postings, You're Next Career Network.\"\n",
    "        ),\n",
    "        \"metadata\": {\"category\": \"job_postings\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"mscac_interview_training\",\n",
    "        \"type\": \"reference\",\n",
    "        \"text\": (\n",
    "            \"For interview training, the MScAC team recommends checking out (c.f. the \\\"Internship and Academic Supervisor Information\\\" page of the CSC2703 page on Quercus for more details): \"\n",
    "            \"Top Interview Tips video, Career Prep Series – TalentBoard, 9 Tips For How To Make Interview Small Talk - Zippia.\"\n",
    "        ),\n",
    "        \"metadata\": {\"category\": \"interview_training\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"mscac_technical_interviews\",\n",
    "        \"type\": \"reference\",\n",
    "        \"text\": (\n",
    "            \"For technical interview preparation, the MScAC team recommends checking out (c.f. the \\\"Internship and Academic Supervisor Information\\\" page of the CSC2703 page on Quercus for more details): \"\n",
    "        ),\n",
    "        \"metadata\": {\"category\": \"technical_interview\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"mscac_online_presence\",\n",
    "        \"type\": \"reference\",\n",
    "        \"text\": (\n",
    "            \"For building your online presence, the MScAC team recommends checking out (c.f. the \\\"Internship and Academic Supervisor Information\\\" page of the CSC2703 page on Quercus for more details): \"\n",
    "            \"Accelerating Your Career Via Your Online Presence video.\"\n",
    "        ),\n",
    "        \"metadata\": {\"category\": \"career_development\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add the URLs for the resources\n",
    "## Job boards\n",
    "links0 = [\"https://jobs.entrepreneurs.utoronto.ca/jobs\",\n",
    "          \"https://talenthub.vectorinstitute.ai/\",\n",
    "          \"https://mltechniques.com/jobs/\",\n",
    "          \"https://techjobs.marsdd.com/jobs\",\n",
    "          \"https://www1.communitech.ca/companies\",\n",
    "          \"https://torontoai.org/job-postings/\",\n",
    "          \"https://www.yourenext.ca/\"\n",
    "          ]\n",
    "external_resources[0][\"text\"] += \" Links: \" + \", \".join(links0)\n",
    "## Interview training\n",
    "links1 = [\n",
    "    \"https://www.youtube.com/watch?v=HG68Ymazo18\", \n",
    "    \"https://talentboard.io/prep/\",\n",
    "    \"https://www.zippia.com/advice/9-tips-for-interview-small-talk/\"\n",
    "]\n",
    "external_resources[1][\"text\"] += \" Links: \" + \", \".join(links1)\n",
    "## Technical interviews\n",
    "page_names = [\"David Stutz blog\", \"LeetCode\", \"InterviewBit\", \"Acing Data & AI Interviews video\", \"Programming Interviews Exposed book\", \n",
    "            \"Cornell interviews page\", \"HackerRank\", \"Firecode\", \"Gayle Laakmann McDowell resources\", \"Python Tutor\", \"Pramp\", \n",
    "            \"Vault\", \"Simplilearn\", \"Springboard\", \"Top Machine Learning Interview Questions - InterviewBit\", \"MLQuestions GitHub repository\"]\n",
    "links2 = [\n",
    "    \"https://davidstutz.de/how-i-prepared-for-deepmind-and-google-ai-research-internship-interviews-in-2019/\",\n",
    "    \"https://leetcode.com/\",\n",
    "    \"https://www.interviewbit.com/big-data-interview-questions/\",\n",
    "    \"https://www.youtube.com/watch?v=Zlefd0fl7iQ\",\n",
    "    \"https://ebookcentral-proquest-com.myaccess.library.utoronto.ca/lib/utorontocc-ebooks/detail.action?docID=5333089\",\n",
    "    \"https://www.cs.cornell.edu/~xanda/interviews.html\",\n",
    "    \"https://www.hackerrank.com/\",\n",
    "    \"https://firecode.io/\",\n",
    "    \"https://www.gayle.com/consulting\",\n",
    "    \"https://pythontutor.com/\",\n",
    "    \"https://www.pramp.com/#/\",\n",
    "    \"https://www.thebalancemoney.com/top-technical-interview-questions-2061227#toc-top-50-technical-interview-questions\",\n",
    "    \"https://vault.com/blogs/interviewing/29-technical-interview-questions-top-tech-firms-ask-internship-candidates\",\n",
    "    \"https://www.simplilearn.com/tutorials/data-science-tutorial/data-science-interview-questions\",\n",
    "    \"https://www.springboard.com/blog/data-science/machine-learning-interview-questions/\",\n",
    "    \"https://www.interviewbit.com/machine-learning-interview-questions/\",\n",
    "    \"https://github.com/andrewekhalel/MLQuestions\",\n",
    "]\n",
    "for i, (page_name, link) in enumerate(zip(page_names, links2)):\n",
    "    if i % 5 == 0 and i != 0:\n",
    "        # Every 5 resources, start a new section\n",
    "        external_resources[2][\"text\"] += f\"Technical interview external resources #{i//5}: \"\n",
    "    external_resources[2][\"text\"] += f\" {page_name}: {link},\"\n",
    "## Online presence\n",
    "links3 = [\n",
    "    \"https://www.youtube.com/watch?v=MOn2Rsd1Oro\"\n",
    "]\n",
    "external_resources[3][\"text\"] += \" Link: \" + \", \".join(links3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894305b8",
   "metadata": {},
   "source": [
    "## Part 7 : Putting everything together\n",
    "\n",
    "The resources we have are the following: \n",
    "- PDFs : in clean_pdf_docs\n",
    "- PPTs : in clean_ppt_docs\n",
    "- docxs : in clean_docx_docs\n",
    "- calendar : in calendar_info\n",
    "- webpages : in webpage_info\n",
    "- additional MScAC resources : in external_resources\n",
    "\n",
    "#### 7.1 Putting everything into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9d7dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_docs = []\n",
    "\n",
    "def add_to_rag(docs, dtype):\n",
    "    for i, d in enumerate(docs):\n",
    "        metadata = {}\n",
    "        if isinstance(d, dict):\n",
    "            metadata.update(d.get(\"metadata\", {}))\n",
    "            metadata[\"source\"] = d.get(\"source\", None)\n",
    "        metadata[\"type\"] = dtype\n",
    "\n",
    "        text = d[\"text\"] if isinstance(d, dict) and \"text\" in d else str(d)\n",
    "\n",
    "        rag_docs.append({\n",
    "            \"id\": f\"{dtype}_{i}\",\n",
    "            \"text\": text,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "add_to_rag(clean_pdf_docs, \"pdf\")\n",
    "add_to_rag(clean_ppt_docs, \"ppt\")\n",
    "add_to_rag(clean_docx_docs, \"docx\")\n",
    "add_to_rag(calendar_info, \"calendar\")\n",
    "add_to_rag(webpage_info, \"webpage\")\n",
    "add_to_rag(external_resources, \"external_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4e928",
   "metadata": {},
   "source": [
    "#### 7.2 Rechunking to make sure chunk sizes are normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a458c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "final_rag_docs = []\n",
    "for doc in rag_docs:\n",
    "    for i, chunk in enumerate(splitter.split_text(doc[\"text\"])):\n",
    "        final_rag_docs.append({\n",
    "            \"id\": f\"{doc['id']}_chunk{i}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": doc[\"metadata\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6da82d",
   "metadata": {},
   "source": [
    "#### 7.3 Saving to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e21e30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "# Save to JSON, each document as a line\n",
    "# Add today's date to the filename\n",
    "today_str = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "save_path = op.join(OUTPUT_DATA_DIR, f\"rag_ready_docs_{today_str}.jsonl\")\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in final_rag_docs:\n",
    "        f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC2701Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
